
import re
import time
from bs4 import BeautifulSoup
from urllib.request import Request, urlopen
import pymysql

# MySQLæ•°æ®åº“è¿æ¥å‡½æ•°
def connect_to_db():
    return pymysql.connect(
        host='localhost',  # æ•°æ®åº“ä¸»æœºåœ°å€
        user='root',       # ç”¨æˆ·å
        password='',  # å¯†ç 
        database='spider_db', # æ•°æ®åº“å
        charset='utf8mb4'   # å­—ç¬¦é›†
    )

# åˆ›å»ºæ•°æ®åº“è¡¨
def create_table():
    conn = connect_to_db()
    cursor = conn.cursor()
    cursor.execute('''
        CREATE TABLE IF NOT EXISTS products (
            id INT AUTO_INCREMENT PRIMARY KEY,
            name VARCHAR(255),
            price VARCHAR(100),
            release_date VARCHAR(100),
            image_url TEXT
        )
    ''')
    conn.commit()
    cursor.close()
    conn.close()

# æ’å…¥äº§å“æ•°æ®
def insert_product(name, price, release_date, image_url):
    try:
        conn = connect_to_db()
        cursor = conn.cursor()

        # å…ˆåˆ é™¤ 30 å¤©å‰çš„æ—§æ•°æ®
        delete_old_products(cursor)

        sql = '''
        INSERT INTO products (name, price, release_date, image_url)
        VALUES (%s, %s, %s, %s)
        '''
        cursor.execute(sql, (name, price, release_date, image_url))
        conn.commit()
        print("âœ… æ•°æ®æ’å…¥æˆåŠŸ")
    except pymysql.MySQLError as e:
        print(f"âŒ æ•°æ®æ’å…¥å¤±è´¥: {e}")
    finally:
        cursor.close()
        conn.close()


def delete_old_products(cursor):
    """åˆ é™¤ 30 å¤©å‰çš„äº§å“"""
    try:
        sql = "DELETE FROM products WHERE created_at < NOW() - INTERVAL 30 DAY"
        cursor.execute(sql)
        print(f"ğŸ—‘ï¸ å·²åˆ é™¤ 30 å¤©å‰çš„æ•°æ®")
    except pymysql.MySQLError as e:
        print(f"âš ï¸ åˆ é™¤æ—§æ•°æ®å¤±è´¥: {e}")


# ä»·æ ¼æ ¼å¼åŒ–å‡½æ•°
def format_price(price_str):
    match = re.search(r'(\d[\d,]*)', price_str)
    return match.group(1).replace(',', '') if match else "N/A"

# å‘å”®æ—¥æœŸæ ¼å¼åŒ–å‡½æ•°
def format_release_date(date_str):
    match = re.search(r'(\d{4})å¹´(\d{2})æœˆ', date_str)
    return f"{match.group(1)}-{match.group(2)}" if match else "N/A"

# è§£æäº§å“è¯¦æƒ…
def parse_product(product_url):
    print(f"\næ­£åœ¨è·å–äº§å“è¯¦æƒ…ï¼š{product_url}")
    
    try:
        req_detail = Request(product_url, headers={'User-Agent': 'Mozilla/5.0'})
        product_html = urlopen(req_detail).read()
        product_soup = BeautifulSoup(product_html, 'html.parser')

        # æå–äº§å“å›¾ç‰‡
        product_img = "N/A"
        product_img_tag = product_soup.find("div", class_="item_image_selected")
        if product_img_tag:
            img_tag = product_img_tag.find("img")
            if img_tag and img_tag.get('src'):
                product_img = img_tag['src']

        # æå–äº§å“åç§°
        product_name = "N/A"
        product_name_tag = product_soup.find("div", class_="item_overview_detail")
        if product_name_tag:
            h1_tag = product_name_tag.find("h1")
            if h1_tag:
                product_name = h1_tag.get_text(strip=True)

        # æå–äº§å“ä»·æ ¼
        product_price = "N/A"
        product_price_tag = product_soup.find("p", class_="price new_price")
        if product_price_tag:
            product_price = format_price(product_price_tag.get_text(strip=True))

        # æå–äº§å“å‘å”®æ—¥æœŸ
        product_date = "N/A"
        product_date_tag = product_soup.find("span", class_="num")
        if product_date_tag:
            product_date = format_release_date(product_date_tag.get_text(strip=True))

        # è¾“å‡ºçˆ¬å–ä¿¡æ¯
        print(f"äº§å“åç§°ï¼š{product_name}")
        print(f"äº§å“ä»·æ ¼ï¼š{product_price}")
        print(f"äº§å“å‘å”®ï¼š{product_date}")
        print(f"äº§å“å›¾ç‰‡ï¼š{product_img}")

        # å­˜å…¥æ•°æ®åº“
        insert_product(product_name, product_price, product_date, product_img)

    except Exception as e:
        print(f"è·å– {product_url} æ—¶å‘ç”Ÿé”™è¯¯: {e}")

# åˆ›å»ºæ•°æ®åº“è¡¨ï¼ˆåªéœ€è¦æ‰§è¡Œä¸€æ¬¡ï¼‰
create_table()

year = 2024
month = 11

# éå†é¡µç ï¼ˆå‡è®¾æœ‰ 29 é¡µï¼‰
for i in range(1, 30):
    list_url = (f'https://www.animate-onlineshop.jp/products/list.php?smt=Conan&ss=9&sl=80'
                f'&ssy={year}&ssm={month}&nd[]=7&nf=1&pageno={i}')
    print(f"\næ­£åœ¨å¤„ç†åˆ—è¡¨é¡µ: {list_url}")
    
    # è¯·æ±‚åˆ—è¡¨é¡µ
    req = Request(list_url, headers={'User-Agent': 'Mozilla/5.0'})
    html_page = urlopen(req).read()
    soup = BeautifulSoup(html_page, 'html.parser')
    
    # æå–æ‰€æœ‰äº§å“è¯¦æƒ…é“¾æ¥
    product_links = set()
    for a in soup.find_all("a", href=True):
        href = a["href"]
        if "/pn/" in href:
            full_url = ("https://www.animate-onlineshop.jp" + href) if href.startswith("/") else href
            product_links.add(full_url)
    
    print(f"åœ¨ç¬¬ {i} é¡µæ‰¾åˆ° {len(product_links)} ä¸ªäº§å“é“¾æ¥ã€‚")
    
    # éå†äº§å“é“¾æ¥çˆ¬å–è¯¦æƒ…
    for product_url in product_links:
        parse_product(product_url)
        time.sleep(0.5)
    
    time.sleep(1)
